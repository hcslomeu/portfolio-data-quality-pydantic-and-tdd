{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Portfolio Project","text":"<p>This repository showcases a data quality monitoring project using Pandera, which is derived from Pydantic. The primary goal is to demonstrate how to enforce data contracts and perform data transformations in a reliable and testable manner.</p> <p>The files in the <code>app</code> folder contain the core logic for the ETL (Extract, Transform, Load) process, including schema definitions and transformation functions. This documentation provides an overview of the project, the data contracts, the transformations applied, and also demonstrates my skills to generate clear and well design documentation for users.</p>"},{"location":"#workflow","title":"Workflow","text":"<pre><code>graph TD;\n    A[Config Variables] --&gt; B[Read SQL DB];\n    B --&gt; V[Input Schema Validation];\n    V --&gt;|Fail| X[Error Alert];\n    V --&gt;|Success| C[Transform KPIs];\n    C --&gt; Y[Output Schema Validation];\n    Y --&gt;|Fail| Z[Error Alert];\n    Y --&gt;|Success| D[Save to DuckDB];\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>portfolio-data-quality-pydantic-and-tdd/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 etl.py\n\u2502   \u251c\u2500\u2500 etl_infer_schema.py\n\u2502   \u251c\u2500\u2500 read_duckb.py\n\u2502   \u251c\u2500\u2500 schema.py\n\u2502   \u2514\u2500\u2500 schema_email.py\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 pic/\n\u251c\u2500\u2500 sql/\n\u2502   \u251c\u2500\u2500 create_table_products_bronze.sql\n\u2502   \u251c\u2500\u2500 create_table_products_bronze_email.sql\n\u2502   \u251c\u2500\u2500 insert_into_table_bronze.sql\n\u2502   \u251c\u2500\u2500 insert_into_table_bronze_email.sql\n\u2502   \u2514\u2500\u2500 insert_wrong_values_into_tabela_bronze.sql\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_func_etl.py\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"data_contract/","title":"Data Contract","text":"<p>This section defines the data contracts for the project. These contracts are enforced using Pandera schemas to ensure data quality and consistency throughout the ETL process. The schemas define the expected data types, constraints, and structure of the dataframes.</p> <p>               Bases: <code>DataFrameModel</code></p> <p>Schema for validating the structure and quality of product data.</p> <p>This class leverages the Pandera <code>DataFrameModel</code> to define a strict data contract for product-related DataFrames. It serves as a centralized, declarative, and self-documenting guide to the expected data types, constraints, and format of product data.</p> <p>By validating DataFrames against this schema, we ensure data integrity before processing, preventing common data quality issues.</p> <p>Attributes:</p> Name Type Description <code>id_prod</code> <code>int</code> <p>The unique identifier for the product. The value must be between 1 and 10 (inclusive).</p> <code>name_prod</code> <code>str</code> <p>The name of the product.</p> <code>quantity</code> <code>int</code> <p>The available stock quantity of the product. Must be between 20 and 200.</p> <code>price</code> <code>float</code> <p>The price of the product. Must be between 5.0 and 120.0.</p> <code>category</code> <code>str</code> <p>The product category.</p> <code>index</code> <code>int</code> <p>The DataFrame index, which must be between 0 and 9.</p> Config <p>coerce (bool): If True, automatically attempts to cast DataFrame     columns to the data types specified in the schema. strict (bool): If True, the DataFrame must contain exactly the columns     specified in the schema; no more, no less. unique (list[str]): Ensures that all rows are unique across the     combination of the specified columns.</p> Source code in <code>app/schema.py</code> <pre><code>class ProductSchema(pa.DataFrameModel):\n    \"\"\"Schema for validating the structure and quality of product data.\n\n    This class leverages the Pandera `DataFrameModel` to define a strict data\n    contract for product-related DataFrames. It serves as a centralized,\n    declarative, and self-documenting guide to the expected data types,\n    constraints, and format of product data.\n\n    By validating DataFrames against this schema, we ensure data integrity\n    before processing, preventing common data quality issues.\n\n    Attributes:\n        id_prod (int): The unique identifier for the product.\n            The value must be between 1 and 10 (inclusive).\n        name_prod (str): The name of the product.\n        quantity (int): The available stock quantity of the product.\n            Must be between 20 and 200.\n        price (float): The price of the product. Must be between 5.0 and 120.0.\n        category (str): The product category.\n        index (int): The DataFrame index, which must be between 0 and 9.\n\n    Config:\n        coerce (bool): If True, automatically attempts to cast DataFrame\n            columns to the data types specified in the schema.\n        strict (bool): If True, the DataFrame must contain exactly the columns\n            specified in the schema; no more, no less.\n        unique (list[str]): Ensures that all rows are unique across the\n            combination of the specified columns.\n    \"\"\"\n\n    id_prod: int = Field(ge=1, description=\"Unique product identifier\")\n    name_prod: str = Field(description=\"Name of the product\")\n    quantity: int = Field(ge=0, le=500, description=\"Available quantity of the product\")\n    price: float = Field(ge=0.0, le=120.0, description=\"Price of the product\")\n    category: str = Field(description=\"Category of the product\")\n    index: pa.typing.Index[int] = Field(ge=0, description=\"Row index\")\n\n    class Config:\n        # Coerce data to the specified types\n        coerce: bool = True\n        # Do not fail on columns present in the DataFrame but not in the schema\n        strict: bool = True\n        # Name of the schema for identification\n        name: str = \"ProductSchema_DataFrameModel\"\n        # Report all duplicate rows\n        unique: list[str] = [\"id_prod\", \"name_prod\", \"quantity\", \"price\", \"category\"]\n        # Allow duplicate column names\n        unique_column_names: bool = False\n        # Do not add columns missing from the DataFrame\n        add_missing_columns: bool = False\n</code></pre> <p>               Bases: <code>ProductSchema</code></p> <p>Extends ProductSchema to include KPI-specific transformations.</p> <p>This schema inherits all validation rules from <code>ProductSchema</code> and appends columns generated during the transformation process for business analysis. It ensures that the data remains valid after new, calculated fields are added.</p> <p>Attributes:</p> Name Type Description <code>inventory_total_value</code> <code>float</code> <p>The total value of the product's inventory, calculated as quantity * price. Must be non-negative.</p> <code>category_normalized</code> <code>str</code> <p>The product category name, normalized to lowercase for consistent grouping and analysis.</p> <code>availability</code> <code>bool</code> <p>A flag indicating if the product is in stock (True if quantity &gt; 0). <code>coerce=True</code> ensures the output is a proper boolean.</p> Source code in <code>app/schema.py</code> <pre><code>class ProductSchemaKPI(ProductSchema):\n    \"\"\"Extends ProductSchema to include KPI-specific transformations.\n\n    This schema inherits all validation rules from `ProductSchema` and appends\n    columns generated during the transformation process for business analysis.\n    It ensures that the data remains valid after new, calculated fields are\n    added.\n\n    Attributes:\n        inventory_total_value (float): The total value of the product's\n            inventory, calculated as quantity * price. Must be non-negative.\n        category_normalized (str): The product category name, normalized to\n            lowercase for consistent grouping and analysis.\n        availability (bool): A flag indicating if the product is in stock\n            (True if quantity &gt; 0). `coerce=True` ensures the output is a\n            proper boolean.\n    \"\"\"\n\n    inventory_total_value: float = Field(\n        ge=0, description=\"Total value of the inventory (greater than 0)\"\n    )\n    category_normalized: str = Field(description=\"Category of the product for the KPI\")\n    availability: bool = Field(\n        coerce=True, description=\"Flag if is available or not\"\n    )  # coerce will make sure that is bool\n</code></pre>"},{"location":"tests/","title":"Tests","text":"<p>This section provides documentation for the tests implemented in the project. The tests are crucial for ensuring the correctness and reliability of the ETL process. They cover the transformation functions and data validation schemas.</p>"},{"location":"tests/#tests.test_func_etl.sample_dataframe","title":"<code>sample_dataframe()</code>","text":"<p>Fixture to create a sample DataFrame for testing.</p> Source code in <code>tests/test_func_etl.py</code> <pre><code>@pytest.fixture\ndef sample_dataframe() -&gt; pd.DataFrame:\n    \"\"\"Fixture to create a sample DataFrame for testing.\"\"\"\n    data = {\n        \"id_prod\": [1, 2, 3],\n        \"name_prod\": [\"Product A\", \"Product B\", \"Product C\"],\n        \"quantity\": [10, 0, 5],\n        \"price\": [2.5, 10.0, 0.0],\n        \"category\": [\"Category A\", \"CATEGORY B\", \"Category C\"],\n    }\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"tests/#tests.test_func_etl.test_availability_calculation","title":"<code>test_availability_calculation(sample_dataframe)</code>","text":"<p>Test the calculation of the 'availability' column.</p> Source code in <code>tests/test_func_etl.py</code> <pre><code>def test_availability_calculation(sample_dataframe):\n    \"\"\"\n    Test the calculation of the 'availability' column.\n    \"\"\"\n    # Use .copy() to ensure the original fixture is not modified\n    transformed_df = transform(sample_dataframe.copy())\n    expected_values = pd.Series([True, False, True], name=\"availability\")\n    pd.testing.assert_series_equal(transformed_df[\"availability\"], expected_values)\n</code></pre>"},{"location":"tests/#tests.test_func_etl.test_category_normalization","title":"<code>test_category_normalization(sample_dataframe)</code>","text":"<p>Test the normalization of the 'category' column to lowercase.</p> Source code in <code>tests/test_func_etl.py</code> <pre><code>def test_category_normalization(sample_dataframe):\n    \"\"\"\n    Test the normalization of the 'category' column to lowercase.\n    \"\"\"\n    # Use .copy() to ensure the original fixture is not modified\n    transformed_df = transform(sample_dataframe.copy())\n    expected_values = pd.Series(\n        [\"category a\", \"category b\", \"category c\"], name=\"category_normalized\"\n    )\n    pd.testing.assert_series_equal(\n        transformed_df[\"category_normalized\"], expected_values\n    )\n</code></pre>"},{"location":"tests/#tests.test_func_etl.test_inventory_total_value_calculation","title":"<code>test_inventory_total_value_calculation(sample_dataframe)</code>","text":"<p>Test the calculation of the 'inventory_total_value' column.</p> Source code in <code>tests/test_func_etl.py</code> <pre><code>def test_inventory_total_value_calculation(sample_dataframe):\n    \"\"\"\n    Test the calculation of the 'inventory_total_value' column.\n    \"\"\"\n    # Use .copy() to ensure the original fixture is not modified\n    transformed_df = transform(sample_dataframe.copy())\n    expected_values = pd.Series([25.0, 0.0, 0.0], name=\"inventory_total_value\")\n    pd.testing.assert_series_equal(\n        transformed_df[\"inventory_total_value\"], expected_values\n    )\n</code></pre>"},{"location":"transformations/","title":"Transformations","text":"<p>This section documents the transformation functions used in the ETL process. These functions are responsible for cleaning, enriching, and reshaping the data to meet the requirements of the target schema.</p> <p>Loads database connection settings from a .env file.</p> <p>This function reads the .env file in the current working directory to load essential database credentials and connection information.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the database settings (host, user, password, database name, and port).</p> Source code in <code>app/etl.py</code> <pre><code>def load_settings():\n    \"\"\"Loads database connection settings from a .env file.\n\n    This function reads the .env file in the current working directory to\n    load essential database credentials and connection information.\n\n    Returns:\n        dict: A dictionary containing the database settings (host, user,\n            password, database name, and port).\n    \"\"\"\n    dotenv_path = Path.cwd() / \".env\"\n    load_dotenv(dotenv_path=dotenv_path)\n\n    settings = {\n        \"db_host\": os.getenv(\"POSTGRES_HOST\"),\n        \"db_user\": os.getenv(\"POSTGRES_USER\"),\n        \"db_pass\": os.getenv(\"POSTGRES_PASSWORD\"),\n        \"db_name\": os.getenv(\"POSTGRES_DB\"),\n        \"db_port\": os.getenv(\"POSTGRES_PORT\"),\n    }\n    return settings\n</code></pre> <p>Executes a SQL query and validates the output against a Pandera schema.</p> <p>This function establishes a connection to the database using the loaded settings, runs the provided SQL query, and fetches the results into a pandas DataFrame. The output DataFrame is then validated against the <code>ProductSchema</code> to ensure data quality and integrity before further processing.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the query results, validated against the <code>ProductSchema</code>.</p> <p>Raises:</p> Type Description <code>SchemaError</code> <p>If the DataFrame loaded from the database fails validation against the <code>ProductSchema</code>.</p> Source code in <code>app/etl.py</code> <pre><code>@pa.check_output(ProductSchema)\ndef run_query(query: str) -&gt; pd.DataFrame:\n    \"\"\"Executes a SQL query and validates the output against a Pandera schema.\n\n    This function establishes a connection to the database using the loaded\n    settings, runs the provided SQL query, and fetches the results into a\n    pandas DataFrame. The output DataFrame is then validated against the\n    `ProductSchema` to ensure data quality and integrity before further\n    processing.\n\n    Args:\n        query (str): The SQL query to execute.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the query results, validated\n            against the `ProductSchema`.\n\n    Raises:\n        pa.errors.SchemaError: If the DataFrame loaded from the database\n            fails validation against the `ProductSchema`.\n    \"\"\"\n\n    settings = load_settings()\n\n    connection_string = f\"postgresql://{settings['db_user']}:{settings['db_pass']}@{settings['db_host']}:{settings['db_port']}/{settings['db_name']}\"\n\n    engine = create_engine(connection_string)\n\n    with engine.connect() as conn, conn.begin():\n        df_crm = pd.read_sql(query, conn)\n\n    return df_crm\n</code></pre> <p>Applies business logic and transformations to the product DataFrame.</p> <p>This function enriches the input DataFrame by: 1.  Calculating the total inventory value for each product. 2.  Normalizing the product category names to lowercase. 3.  Determining product availability based on quantity.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing validated product data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The transformed DataFrame, ready for loading or further analysis.</p> Source code in <code>app/etl.py</code> <pre><code>@pa.check_input(ProductSchema)\n@pa.check_output(ProductSchemaKPI)\ndef transform(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Applies business logic and transformations to the product DataFrame.\n\n    This function enriches the input DataFrame by:\n    1.  Calculating the total inventory value for each product.\n    2.  Normalizing the product category names to lowercase.\n    3.  Determining product availability based on quantity.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing validated product\n            data.\n\n    Returns:\n        pd.DataFrame: The transformed DataFrame, ready for loading or further\n            analysis.\n    \"\"\"\n    df[\"inventory_total_value\"] = df[\"quantity\"] * df[\"price\"]\n    df[\"category_normalized\"] = df[\"category\"].str.lower()\n    df[\"availability\"] = df[\"quantity\"] &gt; 0\n\n    return df\n</code></pre> <p>Carrega o DataFrame no DuckDB, criando ou substituindo a tabela especificada.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame do Pandas para ser carregado no DuckDB.</p> required <code>table_name</code> <code>str</code> <p>Nome da tabela no DuckDB onde os dados ser\u00e3o inseridos.</p> required <code>db_file</code> <code>str</code> <p>Caminho para o arquivo DuckDB. Se n\u00e3o existir, ser\u00e1 criado.</p> <code>'my_duckdb.db'</code> Source code in <code>app/etl.py</code> <pre><code>@pa.check_input(ProductSchemaKPI, lazy=True)\ndef load_to_duckdb(df: pd.DataFrame, table_name: str, db_file: str = \"my_duckdb.db\"):\n    \"\"\"\n    Carrega o DataFrame no DuckDB, criando ou substituindo a tabela especificada.\n\n    Args:\n        df: DataFrame do Pandas para ser carregado no DuckDB.\n        table_name: Nome da tabela no DuckDB onde os dados ser\u00e3o inseridos.\n        db_file: Caminho para o arquivo DuckDB. Se n\u00e3o existir, ser\u00e1 criado.\n    \"\"\"\n    con = duckdb.connect(database=db_file, read_only=False)\n    con.register(\"df_temp\", df)\n    con.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df_temp\")\n    con.close()\n</code></pre>"}]}